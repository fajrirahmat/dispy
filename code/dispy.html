<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> 
  <head> 
    <meta http-equiv="content-type" content="text/html; charset=utf-8" /> 
    <meta name="description" content="Python module for distributing computations across multiple processors on a single machine, among many machines in a cluster or grid. The computations can be standalone programs or python functions." /> 
    <meta name="keywords" content="dispy, python, parallel processing, distributed computing, cluster computing" /> 
    <title> 
      dispy - Distributed and Parallel Computing with Python
    </title> 

<link rel="stylesheet" type="text/css" href="style.css" />
 
  </head> 
  <body> 
    <div id="page"> 
      <center><div class="title">dispy : Python Framework for Distributed and Parallel Computing</div></center>
      <div id="menu">
	<ul>
          <li><a href="http://sourceforge.net/projects/dispy/files">Download</a></li> 
          <li><a href="http://sourceforge.net/projects/dispy/">Project Details</a></li> 
          <li><a href="http://sourceforge.net/p/dispy/discussion">Forums</a></li>
	</ul>
	<hr />
	<ul>
          <li><a href="index.html">Project Page</a></li> 
          <li><a href="dispynode.html">dispynode</a></li> 
          <li><a href="dispyscheduler.html">dispyscheduler</a></li> 
          <li><a href="dispynetrelay.html">dispynetrelay</a></li>
	</ul>
      </div> 

    <div id="content"> 
      <center><h2>dispy</h2></center>
<p>
  While dispy and other components have various options that cover
  rather comprehensive use cases, making it seem complex, most of the
  options have default values that likely work for common cases. For
  example, starting 'dispynode.py' program on each of the nodes on a
  local network and using JobCluster with <tt>computation</tt>, and
  possibly <tt>depends</tt> parameters may be sufficient.
  </p>

<p>
  There are two ways to create clusters with
  dispy: <tt>JobCluster</tt> and <tt>SharedJobCluster</tt>. If only
  one instance of dispy may be running at anytime,
  <tt>JobCluster</tt> is simple to use; it already contains a
  scheduler that will schedule jobs to nodes running 'dispynode'. If,
  however, multiple programs using dispy may be running at anytime,
  <tt>JobCluster</tt> cannot be used - each of the schedulers in each
  instance of dispy will assume the nodes are controlled exclusively
  by each, causing conflicts. Instead, <tt>SharedJobCluster</tt> must
  be used. In this
  case, <a href="dispyscheduler.html">dispyscheduler</a> must also be
  running on some computer and <tt>SharedJobCluster</tt> must
  set <tt>scheduler_node</tt> parameter with the node running
  dispyscheduler (default is the host that
  calls <tt>SharedJobCluster</tt>).
  </p>

<p>Once an instance of JobCluster/SharedJobCluster is created, it can
be used to submit jobs by specifying arguments to invoke the
computations.
</p>
  
<h4>JobCluster</h4>

<code>JobCluster(computation, nodes=['*'], depends=[], callback=None, ip_addr=None, ext_ip_addr=None,
           port=51347, node_port=51348, fault_recover=False, dest_path=None, loglevel=logging.WARNING,
           cleanup=True, pulse_interval=None, ping_interval=None, reentrant=False,
           secret='', keyfile=None, certfile=None)
</code>
where
<ul>
<li>
  <tt>computation</tt> is either a Python function or a string. If it
  is a string, it must be path to executable program. This computation
  is sent to nodes in the given cluster. When a job is submitted (to
  invoke computation with arguments), dispynode executes the
  computation with those arguments in isolation - the computation
  should not depend on global state, such as modules imported in the
  main program or global variables etc.
</li>
<li>
  <tt>nodes</tt> is list. Each element must be either a string or a
  pair (tuple of two elements). If element is a string, it must be
  either IP address or host name. If element is a pair, first element
  of pair must be IP address or name and second element must be port
  number where that node is serving (needed if it is different from
  default 51348). This list serves two purposes: dispy initially sends
  a request to all the nodes listed to find out information about them
  (e.g., number of processing units available for dispy), then sends
  given computation to only those nodes that match the listed nodes
  (dispy may know about nodes not listed in this computation, as it
  also broadcasts identification request).  Wildcard '*' can be used
  to match (part of) any IP address; e.g., '192.168.3.*' matches any
  node whose IP address starts with '192.168.3'.  If there are any
  nodes beyond local network, then all such nodes should be mentioned
  in <tt>nodes</tt>. If there are many such nodes (on outside local
  network), it may be cumbersome to list them all (it is not possible
  to send identification request to outside networks with wildcard in
  them); in that case, <a href="dispynetrelay.html">dispynetrelay</a>
  may be started on one of the nodes on that network and the node
  running dispynetrelay should be added to nodes list (and a wildcard
  for that network, so that other nodes on that network match that
  wildcard); see below for examples.
</li>
<li>
  <tt>depends</tt> is list of dependencies needed
  for <tt>computation</tt>. Each element of this list can be either
  Python function or Python class or an instance of class (object) or
  a Python module or path to file. Only Python modules that are not
  present on nodes already need be listed; standard modules that are
  present on all nodes do not need to be listed here.
</li>
<li>
  <tt>callback</tt> is a function. When a job's results become
  available, dispy will call provided callback function with that job
  as the argument. If a job sends provisional results with
  'dispy_provisional_result' multiple times, then dispy will call
  provided callback each such time. The (provisional) results of
  computation can be retrieved with 'result' field of job, etc. While
  computations are run on nodes in isolated environments, callbacks
  are run in the context of user programs from which
  (Shared)JobCluster is called - for example, callbacks can access
  global variables in programs that created cluster(s).
</li>
<li>
  <tt>ip_addr</tt> is IP address to use for (client) communication. If
  it is not set, IP address set for the host calling JobCluster is
  used. If the host has multiple interfaces and default address is not
  the right choice, this parameter can be set to correct address.
</li>
<li>
  <tt>ext_ip_addr</tt> is external IP address to use for (client)
  communication. This may be needed in case the client is behind a NAT
  firewall/gateway and (some) nodes are outside. Typically, in such a
  case, ext_ip_addr must be the address of NAT firewall/gateway and
  the NAT firewall/gateway must forward ports to ip_addr
  appropriately. See below for more information.
</li>
<li>
  <tt>port</tt> is port to use for (client) communication. Usually not
  necessary. If not given, dispy will request socket library to choose
  any available port.
</li>
<li>
  <tt>node_port</tt> is port to use for communicating with nodes
  (servers). If this is different from default, 'dispynode' programs
  must be run with the same port.
</li>
<li>
  <tt>dest_path</tt> is directory on the nodes where files are
  transferred to. Default is to create a separate directory for each
  computation. If a computation transfers files (dependencies) and
  same computation is run again with same files, the transfer can be
  avoided by specifying same dest_path, along with the option
  'cleanup=False'.
</li>
<li>
  <tt>fault_recover</tt> must be either True or a string. If it is a
  string, dispy uses it as path to file where it stores information
  about jobs scheduled but not finished yet. In case user program
  terminates unexpectedly (for example, because of network failure,
  uncaught exception), the results of submitted jobs can be later
  retrieved through 'fault_recover_jobs' function (see below). If this
  option is True, dispy will store information about jobs in a file of
  the form '_dispy_fault_recover_YYYYMMDDHHMMSS' in the current
  directory. Note that dispy keeps information about only the jobs
  that have been submitted for execution but not finished yet. Once a
  job is finished (i.e., job result is received by dispy's scheduler),
  its information is lost. If it is necessary to keep the information
  about finished jobs, callbacks can be used to persist job results.
</li>
<li>
  <tt>loglevel</tt> is message priority for logging module.
</li>
<li>
  <tt>cleanup</tt>: Whether any files transferred should be deleted
  after the computation is done. If it is False, the files are left on
  the nodes; this may speedup if same files are needed for another
  cluster later. However, this can be security risk and/or require
  manual cleanup. If same files are used for multiple clusters, then
  cleanup may be set to False and same <tt>dest_path</tt> used.
</li>
<li>
  <tt>pulse_interval</tt> is number of seconds between 'pulse'
  messages that nodes send to indicate they are alive and computing
  submitted jobs. If this value is given as an integer or floating
  number between 1 and 600, then a node is presumed dead if
  5*pulse_interval seconds elapse without a pulse message. See
  'reentrant' below.
</li>
<li>
  <tt>reentrant</tt> must be either True or False. This value is used
  only if 'pulse_interval' is set for any of the clusters. If
  pulse_interval is given and reentrant is False (default), jobs
  scheduled for a dead node are automatically cancelled (for such jobs,
  execution result, output and error fields are set to None, exception
  field is set to 'Cancelled' and status is set to Cancelled); if
  reentrant is True, then jobs scheduled for a dead node are resubmitted
  to other available nodes.
</li>
<li>
  <tt>ping_interval</tt> is number of seconds.  Normally dispy can
  locate nodes running dispynode by broadcasting UDP ping messages on
  local network and point-to-point UDP messages to nodes on remote
  networks. However, UDP messages may get lost.  Ping interval is
  number of seconds between repeated ping messages to find any nodes
  that have missed previous ping messages.
  </li>
<li>
  <tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with nodes. This prevents unauthorized
  use of nodes. However, the hashed string (not the secret itself) is
  passed in clear text, so an unauthorized, determined person may be
  able to figure out how to circumvent.
</li>
<li>
  <tt>keyfile</tt> is path to file containing private key for SSL
  communication (see Python 'ssl' module). This key may be stored in
  'certfile' itself, in which case this should be None.
</li>
<li>
  <tt>certfile</tt> is path to file containing SSL certificate (see Python
  'ssl' module).
</li>
</ul>

<h4>SharedJobCluster</h4>
<p>
  SharedJobCluster has almost the same syntax, except as noted below.
  </p>
<code>SharedJobCluster(computation, nodes=['*'], depends=[], ip_addr=None, ext_ip_addr=None, scheduler_node=None,
                 port=51347, dest_path=None, loglevel=logging.WARNING, cleanup=True, reentrant=False,
                 secret='', keyfile=None, certfile=None)
</code>
where all arguments common to <tt>JobCluster</tt> are same, and
<ul>
<li>
  <tt>scheduler_node</tt> is either IP address or host name
  where <a href="dispyscheduler.html">dispyscheduler</a> is running; if
  it is not given, the node where SharedJobCluster is invoked is
  used
</li>
<li>
  <tt>pulse_interval</tt> is not used in case of SharedJobCluster;
  instead, 'dispyscheduler' must be called with '--pulse_interval'
  option appropriately.
</li>
<li>
  <tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with dispyscheduler.
</li>
</ul>

A cluster has following methods:
<ul>
<li>
  <tt>cluster.submit</tt> method is available for the cluster returned
  from JobCluster. This method should be called with the arguments
  exactly as expected by the 'computation' given to JobCluster. If
  'computation' is a Python function, the arguments may also contain
  keyword arguments. However, all arguments must be serializable
  (picklable). If an argument is a class object that contains
  non-serializable members, then the classes may
  provide <tt>__getstate__</tt> method for this purpose (see '_Job'
  class in dispy.py for an example). If 'computation' is a program, then
  all arguments must be strings.

  <tt>submit</tt> returns a 'job' object. Results from execution of
  computation with given arguments will be available in the 'job' object
  after execution finishes.
</li>

<li>
  <tt>cluster()</tt> will wait for all submitted jobs to complete.
</li>

<li>
  <tt>cluster.wait()</tt> will wait for all submitted jobs to complete.
</li>

<li>
  <tt>cluster.close()</tt> will wait for all submitted jobs to complete
  and then cleanup (such as removing any transferred files, deleting
  'computation' from the nodes etc.).
</li>

<li>
  <tt>cluster.cancel(job)</tt> will remove the job submitted, by
  terminating it if it already started execution. Note that if the job
  execution has any side effects (such as updating database, files
  etc.), cancelling a job may leave unpredictable side effects,
  depending on at what point job is cancelled.
</li>

<li>
  <tt>cluster.stats()</tt> will print statistics about nodes, time
  each node spent executing jobs etc.
</li>
</ul>

<h4>DispyJob</h4>

<p>
 The result of <tt>submit</tt> call of a cluster is an instance of
DispyJob (see dispy.py), which can be used to examine status of job
execution, retrieve job results etc. The job instance has <tt>id</tt>
field that can be used to set any value appropriate (rest of the
fields are either read-only, or not meant for user programs). For
example, <tt>id</tt> field can be set to a unique value to distinguish
one job from another.
</p>

<p>
Job's <tt>status</tt> field is read-only field; its value is one of
<tt>Created</tt>, <tt>Running</tt>, <tt>Finished</tt>, <tt>Cancelled</tt>
or <tt>Terminated</tt>, indicating current status of job.  If job is
created for SharedJobCluster, <tt>status</tt> is not updated to
<tt>Running</tt> when job is actually running.
</p>
  
<p>
When a submitted job is called with job(), it returns that job's
execution result, possibly waiting until the job is finished. After a
job is complete,
</p>
<ul>
  <li>job.result will have computation's result - return value if
  computation is a function and exit status if it is a
  program. job.result is same as return value of job() </li>
  <li>job.stdout and job.stderr will have stdout and stderr strings</li>
  <li>job.exception will have exception trace if computation raises
    any exception; in this case the result of job.result will
    be <tt>None</tt></li>
  <li>job.start_time will be the time when job is scheduled for execution on a node</li>
  <li>job.end_time will be time when results became available</li>
  </ul>

Job's result, stdout and stderr should not be large - these are
buffered and hence will consume memory (not stored on disk). Moreover,
like args and kwargs, result should be serializable (picklable
object). If result is (or has) an instance of python class, that class
may have to provide __getstate__ function to serialize the object.

<p>
After jobs are submitted, cluster.wait() can be used to wait until all
submitted jobs for that cluster have finished. If necessary, results
of execution can be retrieved by either job() or job.result, as
described above.
</p>


<h4>Fault Recovery</h4>
<p>
As noted above, if 'fault_recover' option is used when creating a
cluster, dispy stores information about scheduled but unfinished jobs
in a file. If user program then terminates unexpectedly, the nodes
that execute those jobs can't send the results back to dispy. In such
cases, the results for the jobs can be retrieved from the nodes with
the function in dispy</p>
<tt>fault_recover_jobs(fault_recover_file, ip_addr=None, secret='', node_port=51348,
                       certfile=None, keyfile=None)</tt>
where
<ul>
  <li><tt>fault_recover_file</tt> is path to the file used or created
  when JobCluster is used.</li>
  <li><tt>ip_addr</tt> is IP address to use for (client)
    communication. This may be needed in case the client has multiple
    interfaces and default interface is not the right choice (this
    would be same as the 'ip_addr' option used for JobCluster).</li>
  <li><tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with nodes (should same as the one used
  when creating JobCluster).</li>
  <li><tt>node_port</tt> is port to use for communicating with nodes
  (servers). If this should be different from default, 'dispynode'
  programs must be run with the same port. This option should be same
  as the one used when creating JobCluster.</li>
  <li><tt>certfile</tt> is path to file containing SSL certificate (see Python
  'ssl' module) (same as the one used when creating JobCluster).</li>
  <li><tt>keyfile</tt> is path to file containing private key for SSL
  communication (see Python 'ssl' module). This key may be stored in
  'certfile' itself, in which case this should be None. This option
  should be same as the one used when creating JobCluster.</li>
</ul>

<p>
This function reads the information about jobs in the
fault_recover_file, retrieves DispyJob instance (that contains
results, stdout, stderr, status etc.) for each job that was scheduled
for execution but unfinished at the time of crash, and returns them as
a list. If a job has finished executing at the time
'fault_recover_jobs' function is called, the information about that is
deleted from both the node and fault_recover_file, so the results for
finished jobs can't be retrieved more than once. However, if a job is
still executing, the status field of DispyJob would be
DispyJob.Running and the results for this job can be retrieved again
(until that job finishes) by calling 'fault_recover_jobs'. Note that
'fault_recover_jobs' is available as separate function - it doesn't
need JobCluster or SharedJobCluster instance. In fact,
'fault_recover_jobs' function must not be used when a cluster that
uses same recover file is currently running.
</p>

<p>
Note that dispy sends only the given computation and its dependencies
to the nodes; the program itself is not transferred. So if computation
is a python function, it must import all the modules used by it, even
if the program imported those modules before cluster is created.
</p>

<h4>Provisional Results</h4>
<p>
  'dispy_provisional_result' function can be used in computations
  (Python functions) to send provisional results back to the
  client. For example, in optimization computations, there may be many
  (sub) optimal results that the computations can inform the client
  (program) that may cancel computations, or create additional
  computations, etc. 'dispy_provisional_result' can be used to send
  any information, any number of times, back to the client. As an
  illustrative example, consider:
  </p>
<code>#!/usr/bin/env python

import random, dispy

def compute(n, threshold):
    import random, time, socket
    name = socket.gethostname()
    for i in xrange(0, n):
        r = random.uniform(0, 1)
        if r <= threshold:
            # possible result
            dispy_provisional_result((name, r))
        time.sleep(0.1)
    # final result
    return None

def job_callback(job):
    # callback is called for Terminated status, too, in which case
    # job.result would be None
    if job.result is not None:
        if job.result[1] < 0.005:
            # acceptable result; terminate jobs
            print '%s computed: %s' % (job.result[0], job.result[1])
            global jobs, cluster
            for j in jobs:
                if j.status == dispy.DispyJob.ProvisionalResult:
                    cluster.cancel(j)

if __name__ == '__main__':
    cluster = dispy.JobCluster(compute, callback=job_callback)
    jobs = []
    for n in xrange(4):
        job = cluster.submit(random.randint(50,100), 0.2)
        if job is None:
            print 'creating job %s failed!' % n
            continue
        job.id = n
        jobs.append(job)
    cluster.wait()
    cluster.stats()
    cluster.close()
  </code>
<p>
  In the above example, computations send provisional result if
  computed number is &lt;= threshold (0.2). If the number computed is &lt;
  0.005, job_callback deems it acceptable and terminates computations.
  </p>

<h4>NAT/Firewall Forwarding</h4>
<p>
By default dispy client uses UDP and TCP ports 51347, dispynode uses
UDP and TCP ports 51348, and dispyscheduler uses UDP and TCP pots
51347 and TCP port 51348. If client/node/scheduler are behind a NAT
firewall/gateway, then these ports must be forwarded appropriately and
'ext_ip_addr' option must be used. For example, if dispy client is
behdind NAT firewall/gateway, JobCluster/SharedJobCluster must set
'ext_ip_addr' to the NAT firewall/gateway address and forward UDP and
TCP ports 51347 to the IP address where client is running. Similarly,
if dispynode is behind NAT firewall/gateway, 'ext_ip_addr' option must
be used.
  </p>

<h4>Cloud Computing with Amazon EC2</h4>
<p>
  <tt>ext_ip_addr</tt> option can be used to work with Amazon EC2
  cloud computing service. With EC2 service, a node has a private IP
  address (called 'Private DNS Address') that uses private network of
  the form 10.x.x.x and public address (called 'Public DNS Address')
  that is of the form ec2-x-x-x-x.x.amazonaws.com. After launching
  instance(s), one can copy dispy files to the node(s) and run
  dispynode as
  <tt>dispynode.py --ext_ip_addr ec2-x-x-x-x.y.amazonaws.com</tt>
  (this address can't be used with '-i'/'--ip_addr' option, as the
  network interface is configured with private IP address only). This
  node can then be used by dispy client from outside EC2 network by
  specifying ec2-x-x-x-x.x.amazonaws.com in the 'nodes' list (thus,
  using EC2 servers to augment processing units). Roughly, dispy uses
  'ext_ip_addr' similar to NAT - it announces 'ext_ip_addr' to
  other services instead of the configured 'ip_addr' so that external
  services send requests to 'ext_ip_addr' and if firewall/gateway
  forwards them appropriately, dispy will process them.
  </p>

<h4>Examples</h4>
<p>
  Below are some examples on various use cases of creating clusters:
</p>
  <ol>
    <li><tt>cluster = dispy.JobCluster(compute, depends=[ClassA, moduleB, 'file1'])</tt><br />
      distributes 'compute' along with ClassA (Python object), moduleB (Python object) and
      'file1'. Presumably ClassA, moduleB and file1 are needed by 'compute'.</li>

    <li><tt>cluster = dispy.JobCluster('/some/program', nodes=['192.168.3.*'])</tt><br />
      distributes '/some/program' (an executable program, instead of Python function)
      to all nodes whose IP address starts with '192.168.3'.</li>

    <li><tt>cluster = dispy.JobCluster(compute, nodes=['node20', '192.168.2.21', 'node24'])</tt>
      sends computation to nodes 'node20', 'node24' and node with IP address '192.168.2.21';
      in this case, these nodes could be in different networks, as explicit names / IP addresses
      are listed.</li>

    <li>
      <tt>cluster = dispy.JobCluster(compute, nodes=['192.168.2.*'])</tt>
      sends computation to all nodes whose IP address starts with '192.168.2'.
      In this case, it is assumed that '192.168.2' is local network (since
      dispy can't send identification request to outside networks with wildcard)
    </li>

    <li>
      <code>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.3.22',
                                           '172.16.11.22', 'node39', '192.168.2.*'])</code>
      sends computation to nodes with IP addresses '192.168.3.5', '192.168.3.22', '172.16.11.22' and node 'node39' (since explicit names / IP addresses are listed, they could be on different networks), all nodes whose IP address starts with '192.168.2' (local network).
    </li>

    <li>
      <code>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.3.*', '192.168.2.*'])</code>
      In this case, dispy will send identification request to node with IP address '192.168.3.5'.
      If this node is running 'dispynetrelay', then all the nodes on that network are eligible for
      executing this computation, as wildcard '192.168.3.*' matches IP addresses of those nodes.
      In addition, computation is also sent to all nodes whose IP address starts with '192.168.2'
      (local network).
    </li>

    <li>
      <code>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.8.20',
                                            '172.16.2.99', '*'])</code>
      In this case, dispy will send identification request to nodes with IP address '192.168.3.5',
      '192.168.8.20' and '172.16.2.99'. If these nodes all are running dispynetrelay, then all
      the nodes on those networks are eligible for executing this computation, as wildcard '*'
      matches IP addresses of those nodes. In addition, computation is also sent to all nodes on
      local network (since they also match wildcard '*' and identification request is broadcast
      on local network).
    </li>

    <li>
      Assuming that 192.168.1.39 is the (private) IP address where
      dispy client is used, a.b.c.d is the (public) IP address of NAT
      firewall/gateway (that can be reached from outside) and
      dispynode is running at another public IP address e.f.g.h (so
      that a.b.c.d and e.f.g.h can communicate, but e.f.g.h can't
      communicate with 192.168.1.39),<br />
      <code>cluster = dispy.JobCluster(compute, ip_addr='192.168.1.39', ext_ip_addr='a.b.c.d',
	                               nodes=['e.f.g.h'])</code>
      would work if NAT firewall/gateway forwards UDP and TCP ports 51347 to
      192.168.1.39.
      </li>

    <li><tt>cluster = dispy.JobCluster(compute, secret='super') </tt><br />
      distributes 'compute' to nodes that also use secret 'super' (i.e.,
      nodes started with 'dispynode -s super')<br/> Note that secret is
      used only for establishing communication initially, but not used
      to encrypt programs or code for python objects. This can be useful
      to prevent other users from (inadvertantly) using the nodes. If
      encryption is needed, use SSL; see below.</li>

    <li><tt>cluster = dispy.JobCluster(compute, certfile='mycert',
	keyfile='mykey')</tt><br /> distributes 'compute' and encrypts all
      communication using SSL certificate stored in 'mycert' file and key
      stored in 'mykey' file. In this case, dispynode must also use same
      certificate and key; i.e., each dispynode must be invoked with
      <tt>dispynode --certfile="mycert" --keyfile="mykey"'</tt><br/>

      If both certificate and key are stored in same file, say,
      'mycertkey', they are expected to be in certfile:<br />
      <tt>cluster = dispy.JobCluster(compute, certfile='mycertkey')</tt></li>

    <li><tt>cluster1 = dispy.JobCluster(compute1, nodes=['192.168.3.2', '192.168.3.5'])</tt><br/>
      <tt>cluster2 = dispy.JobCluster(compute2, nodes=['192.168.3.10',
	'192.168.3.11'])</tt><br /> distributes 'compute1' to nodes
      192.168.3.2 and 192.168.3.5, and 'compute2' to nodes 192.168.3.10
      and 192.168.3.11. With this setup, specific computations can be
      scheduled on certain node(s). As mentioned above, with JobCluster,
      the set of nodes for one cluster must be disjoint with set of
      nodes in any other cluster running at the same time. Otherwise,
      <a href="dispy.html">SharedJobCluster</a> must be used.</li>

  </ol>

      </div> 
    </div> 
    <div id="footer"> 
      <p> 
        <a href="http://sourceforge.net/"> 
          Project Web Hosted by <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=539226&amp;type=3" alt="SourceForge.net" /> 
        </a> 
      </p> 
      <p> 
        &copy;Copyright 1999-2009 -
        <a href="http://geek.net" title="Network which provides and promotes Open Source software downloads, development, discussion and news."> 
          Geeknet</a>, Inc., All Rights Reserved
      </p> 
      <p> 
        <a href="http://sourceforge.net/about"> 
          About
        </a> 
        -
        <a href="http://sourceforge.net/tos/tos.php"> 
          Legal
        </a> 
        -
        <a href="http://p.sf.net/sourceforge/getsupport"> 
          Help
        </a> 
      </p> 
    </div> 

  </body> 
</html> 
